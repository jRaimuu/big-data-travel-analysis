{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.4.1 google-cloud-storage hadoop-aws==3.3.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RtyJObvUXPM",
        "outputId": "6e292457-d60c-4320-a8ba-0350abba4813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.4.1\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement hadoop-aws==3.3.2 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for hadoop-aws==3.3.2\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas requests beautifulsoup4\n",
        "!pip install pygam\n",
        "!pip install tensorflow\n",
        "!pip install scikit-learn\n",
        "!pip install gdown==v4.6.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B8j1BGHIv7t",
        "outputId": "07c598c6-d245-4e84-ab4f-51cfbbcbebf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Collecting pygam\n",
            "  Downloading pygam-0.9.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.10/dist-packages (from pygam) (1.26.4)\n",
            "Requirement already satisfied: progressbar2<5.0.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pygam) (4.5.0)\n",
            "Collecting scipy<1.12,>=1.11.1 (from pygam)\n",
            "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-utils>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from progressbar2<5.0.0,>=4.2.0->pygam) (3.9.1)\n",
            "Requirement already satisfied: typing_extensions>3.10.0.2 in /usr/local/lib/python3.10/dist-packages (from python-utils>=3.8.1->progressbar2<5.0.0,>=4.2.0->pygam) (4.12.2)\n",
            "Downloading pygam-0.9.1-py3-none-any.whl (522 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m522.0/522.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, pygam\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "Successfully installed pygam-0.9.1 scipy-1.11.4\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting gdown==v4.6.3\n",
            "  Downloading gdown-4.6.3-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown==v4.6.3) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown==v4.6.3) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown==v4.6.3) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown==v4.6.3) (4.66.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==v4.6.3) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==v4.6.3) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==v4.6.3) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==v4.6.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==v4.6.3) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==v4.6.3) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==v4.6.3) (1.7.1)\n",
            "Downloading gdown-4.6.3-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 5.2.0\n",
            "    Uninstalling gdown-5.2.0:\n",
            "      Successfully uninstalled gdown-5.2.0\n",
            "Successfully installed gdown-4.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator"
      ],
      "metadata": {
        "id": "xAeSjHNiIr4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from pygam import LinearGAM, s\n",
        "\n",
        "from pyspark.sql.functions import when, col, avg, abs"
      ],
      "metadata": {
        "id": "n6NWlNIGIo4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"TouristDataAnalysis\").getOrCreate()"
      ],
      "metadata": {
        "id": "W0Ux7LnfIwyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-storage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4oE22QpMArH",
        "outputId": "3b1fa23c-2832-455a-d26a-cd3a008cb676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.27.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.32.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.25.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.8.30)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/annual_co2_emissions.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"annual_co2.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "annual_co2 = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"annual_co2.csv\")\n",
        "\n",
        "# Rename the column\n",
        "annual_co2 = annual_co2.withColumnRenamed('Annual COâ\\x82\\x82 emissions', 'annual_co2')\n",
        "\n",
        "# Show the result to verify\n",
        "annual_co2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43rj718fJRyu",
        "outputId": "632b874a-2caf-4c7e-90f2-aeabc8e1cff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to annual_co2.csv.\n",
            "+-----------+----+----+----------+\n",
            "|     Entity|Code|Year|annual_co2|\n",
            "+-----------+----+----+----------+\n",
            "|Afghanistan| AFG|1949|   14656.0|\n",
            "|Afghanistan| AFG|1950|   84272.0|\n",
            "|Afghanistan| AFG|1951|   91600.0|\n",
            "|Afghanistan| AFG|1952|   91600.0|\n",
            "|Afghanistan| AFG|1953|  106256.0|\n",
            "|Afghanistan| AFG|1954|  106256.0|\n",
            "|Afghanistan| AFG|1955|  153888.0|\n",
            "|Afghanistan| AFG|1956|  183200.0|\n",
            "|Afghanistan| AFG|1957|  293120.0|\n",
            "|Afghanistan| AFG|1958|  329760.0|\n",
            "|Afghanistan| AFG|1959|  384571.0|\n",
            "|Afghanistan| AFG|1960|  413885.0|\n",
            "|Afghanistan| AFG|1961|  490798.0|\n",
            "|Afghanistan| AFG|1962|  688594.0|\n",
            "|Afghanistan| AFG|1963|  706736.0|\n",
            "|Afghanistan| AFG|1964|  838551.0|\n",
            "|Afghanistan| AFG|1965| 1006917.0|\n",
            "|Afghanistan| AFG|1966| 1091159.0|\n",
            "|Afghanistan| AFG|1967| 1281865.0|\n",
            "|Afghanistan| AFG|1968| 1223391.0|\n",
            "+-----------+----+----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/annual_ghe.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"annual_ghe.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "annual_ghe = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"annual_ghe.csv\")\n",
        "\n",
        "# Rename the column\n",
        "annual_ghe = annual_ghe.withColumnRenamed('Annual greenhouse gas emissions in COâ\\x82\\x82 equivalents', 'annual_ghe')\n",
        "\n",
        "# Show the result to verify\n",
        "annual_ghe.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXOOzJN-Ur6D",
        "outputId": "7ef1bbf9-d238-404e-ddc8-c3137080b4a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to annual_ghe.csv.\n",
            "+-----------+----+----+----------+\n",
            "|     Entity|Code|Year|annual_ghe|\n",
            "+-----------+----+----+----------+\n",
            "|Afghanistan| AFG|1850| 7435743.5|\n",
            "|Afghanistan| AFG|1851| 7499858.5|\n",
            "|Afghanistan| AFG|1852| 7560495.5|\n",
            "|Afghanistan| AFG|1853| 7619898.0|\n",
            "|Afghanistan| AFG|1854| 7678120.0|\n",
            "|Afghanistan| AFG|1855| 7735004.5|\n",
            "|Afghanistan| AFG|1856| 7790375.5|\n",
            "|Afghanistan| AFG|1857| 7845360.0|\n",
            "|Afghanistan| AFG|1858| 7898627.5|\n",
            "|Afghanistan| AFG|1859| 7950452.5|\n",
            "|Afghanistan| AFG|1860| 8013327.0|\n",
            "|Afghanistan| AFG|1861| 8069956.0|\n",
            "|Afghanistan| AFG|1862| 8121903.5|\n",
            "|Afghanistan| AFG|1863| 8172425.0|\n",
            "|Afghanistan| AFG|1864| 8221971.0|\n",
            "|Afghanistan| AFG|1865| 8270002.0|\n",
            "|Afghanistan| AFG|1866| 8317355.0|\n",
            "|Afghanistan| AFG|1867| 8364141.5|\n",
            "|Afghanistan| AFG|1868| 8410427.0|\n",
            "|Afghanistan| AFG|1869| 8456380.0|\n",
            "+-----------+----+----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/annual_deforest.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"annual_deforest.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "annual_deforest = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"annual_deforest.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFujC_QS0cxx",
        "outputId": "2eaaba0f-056c-4b19-cb67-d06b85920852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to annual_deforest.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/tree_cover_loss_wildfires.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"tree_cover_loss_wildfires.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "tree_cover_loss_wildfires = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"tree_cover_loss_wildfires.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD8JOlIq0gkI",
        "outputId": "dc0b791d-6b56-47b0-f563-60bef5448901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to tree_cover_loss_wildfires.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/energy_consumption.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"energy_consumption.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "energy_consumption = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"energy_consumption.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obHbGPXg0kUb",
        "outputId": "ef787e45-8f1f-4d9c-c287-0dd90d378d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to energy_consumption.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/average_precipitation.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"average_precipitation.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "average_precipitation = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"average_precipitation.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMDxdm1J0nVz",
        "outputId": "b3d359ed-5931-46d1-fb03-e1f29b2f65a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to average_precipitation.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/gdp_ppp_per_capita.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"gdp_ppp_per_capita.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "gdp_ppp_per_capita = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"gdp_ppp_per_capita.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPSxGJoT0sLj",
        "outputId": "aed58c4a-a284-46a0-b1b6-bb97e37b4d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to gdp_ppp_per_capita.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/gdp_nominal_per_capita.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"gdp_nominal_per_capita.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "gdp_nominal_per_capita = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"gdp_nominal_per_capita.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0oH7fAw0u0Z",
        "outputId": "67730dea-ddb9-4471-c47c-687ac5562c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to gdp_nominal_per_capita.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/inflation_rate.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"inflation_rate.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "inflation_rate = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"inflation_rate.csv\")\n",
        "\n",
        "# Rename the column\n",
        "inflation_rate = inflation_rate.withColumnRenamed('fp_cpi_totl_zg', 'inflation_rate')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QUyr8Ju0xlD",
        "outputId": "7b42a71e-fe4d-42c1-80f1-f86fe8bf8e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to inflation_rate.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/crime_rate.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"crime_rate.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "crime_rate = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"crime_rate.csv\")\n",
        "\n",
        "# Rename the column\n",
        "crime_rate = crime_rate.withColumnRenamed('value__category_total__sex_total__age_total__unit_of_measurement_rate_per_100_000_population', 'crime_rate')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUy8h7ql00i9",
        "outputId": "2269f4e9-53a5-488b-e391-12294a9e486d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to crime_rate.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/internet_penetration_rate.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"internet_penetration_rate.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "internet_penetration_rate = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"internet_penetration_rate.csv\")\n",
        "\n",
        "# Rename the column\n",
        "internet_penetration_rate = internet_penetration_rate.withColumnRenamed('it_net_user_zs', 'internet_penetration_rate')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxiWdiiw03tY",
        "outputId": "bf538b23-3c53-49a0-dae8-5f75c1c76ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to internet_penetration_rate.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/intl_tourist_spending.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"intl_tourist_spending.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "intl_tourist_spending = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"intl_tourist_spending.csv\")\n",
        "\n",
        "# Rename the column\n",
        "intl_tourist_spending = intl_tourist_spending.withColumnRenamed('it_net_user_zs', 'intl_tourist_spending')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jvXa0Ix06n2",
        "outputId": "e533fd5c-ba9c-4a94-9dbc-108f186f1e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to intl_tourist_spending.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/natural_disaster_death.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"natural_disaster_death.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "natural_disaster_death = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"natural_disaster_death.csv\")\n",
        "\n",
        "# Rename the column\n",
        "natural_disaster_death = natural_disaster_death.withColumnRenamed('death_count__age_group_allages__sex_both_sexes__cause_natural_disasters', 'natural_disaster_death')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiJDVeSS0_2Y",
        "outputId": "ab0dd1e8-82fb-4f17-a3d9-36a01ce4f0e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to natural_disaster_death.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/number_of_UNESCO_WHS.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"number_of_UNESCO_WHS.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "number_of_UNESCO_WHS = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"number_of_UNESCO_WHS.csv\")\n",
        "\n",
        "# Select specific columns\n",
        "number_of_UNESCO_WHS = number_of_UNESCO_WHS.select(\"Country\", \"Total sites\")\n",
        "\n",
        "# Rename columns\n",
        "number_of_UNESCO_WHS = number_of_UNESCO_WHS \\\n",
        "    .withColumnRenamed(\"Total sites\", \"number_of_UNESCO_WHS\") \\\n",
        "    .withColumnRenamed(\"Country\", \"Entity\")\n",
        "\n",
        "# Show the result to verify\n",
        "number_of_UNESCO_WHS.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9gpysS11CZ-",
        "outputId": "b87e4c17-551c-47e2-f9d9-1e48787699da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to number_of_UNESCO_WHS.csv.\n",
            "+--------------------+--------------------+\n",
            "|              Entity|number_of_UNESCO_WHS|\n",
            "+--------------------+--------------------+\n",
            "|         Afghanistan|                   2|\n",
            "|             Albania|                   4|\n",
            "|             Algeria|                   7|\n",
            "|             Andorra|                   1|\n",
            "|              Angola|                   1|\n",
            "| Antigua and Barbuda|                   1|\n",
            "|           Argentina|                  12|\n",
            "|             Armenia|                   3|\n",
            "|           Australia|                  20|\n",
            "|             Austria|                  12|\n",
            "|          Azerbaijan|                   5|\n",
            "|             Bahrain|                   3|\n",
            "|          Bangladesh|                   3|\n",
            "|            Barbados|                   1|\n",
            "|             Belarus|                   4|\n",
            "|             Belgium|                  16|\n",
            "|              Belize|                   1|\n",
            "|               Benin|                   3|\n",
            "|             Bolivia|                   7|\n",
            "|Bosnia and Herzeg...|                   5|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/population.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"population.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "population = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"population.csv\")\n",
        "\n",
        "# Select specific columns\n",
        "population = population.select(\"Location\", \"Population\")\n",
        "\n",
        "# Rename columns\n",
        "population = population \\\n",
        "    .withColumnRenamed(\"Location\", \"Entity\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzq7OxMc1FOW",
        "outputId": "dbe4198a-a5d0-4ceb-c780-7c565ecabde5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to population.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/political_stability.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"political_stability.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "political_stability = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"political_stability.csv\")\n",
        "\n",
        "# Select specific columns\n",
        "political_stability = political_stability.select(\"Country\", \"2024 score\")\n",
        "\n",
        "# Rename columns\n",
        "political_stability = political_stability \\\n",
        "    .withColumnRenamed(\"2024 score\", \"political_stability\") \\\n",
        "    .withColumnRenamed(\"Country\", \"Entity\")\n",
        "\n",
        "# Show the result to verify\n",
        "political_stability.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u416B4E1rPd",
        "outputId": "990d0f5c-d6e5-4689-f168-c75101b43dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to political_stability.csv.\n",
            "+--------------------+-------------------+\n",
            "|              Entity|political_stability|\n",
            "+--------------------+-------------------+\n",
            "|             Somalia|              111.3|\n",
            "|               Sudan|              109.3|\n",
            "|         South Sudan|              109.0|\n",
            "|               Syria|              108.1|\n",
            "|            DR Congo|              106.7|\n",
            "|               Yemen|              106.6|\n",
            "|         Afghanistan|              103.9|\n",
            "|Central African R...|              103.9|\n",
            "|               Haiti|              103.5|\n",
            "|                Chad|              102.7|\n",
            "|             Myanmar|              100.0|\n",
            "|            Ethiopia|               98.1|\n",
            "|        Palestine[a]|               97.8|\n",
            "|                Mali|               97.3|\n",
            "|             Nigeria|               96.6|\n",
            "|               Libya|               96.5|\n",
            "|              Guinea|               96.4|\n",
            "|            Zimbabwe|               95.7|\n",
            "|               Niger|               95.2|\n",
            "|            Cameroon|               94.3|\n",
            "+--------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/infrastructure.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"infrastructure.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "infrastructure = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"infrastructure.csv\")\n",
        "\n",
        "# Select specific columns\n",
        "infrastructure = infrastructure.select('Country', 'Overall Infrastructure Score', 'Basic Infrastructure Score', 'Technological Infrastructure Score', 'Scientific Infrastructure Score', 'Health and Environment Score', 'Education Score')\n",
        "\n",
        "# Rename columns\n",
        "infrastructure = infrastructure \\\n",
        "    .withColumnRenamed(\"Country\", \"Entity\")\n",
        "\n",
        "# Show the result to verify\n",
        "infrastructure.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHbjNkrM16Jq",
        "outputId": "6252cb51-1145-40d0-fd34-b39825070815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to infrastructure.csv.\n",
            "+-------------+----------------------------+--------------------------+----------------------------------+-------------------------------+----------------------------+---------------+\n",
            "|       Entity|Overall Infrastructure Score|Basic Infrastructure Score|Technological Infrastructure Score|Scientific Infrastructure Score|Health and Environment Score|Education Score|\n",
            "+-------------+----------------------------+--------------------------+----------------------------------+-------------------------------+----------------------------+---------------+\n",
            "|  Switzerland|                        88.4|                      61.1|                              65.7|                           79.6|                        72.8|           72.2|\n",
            "|      Denmark|                        84.3|                      61.0|                              67.1|                           70.7|                        71.3|           69.1|\n",
            "|       Sweden|                        81.8|                      54.0|                              65.3|                           75.1|                        71.5|           65.7|\n",
            "|    Singapore|                        77.1|                      61.1|                              70.4|                           66.8|                        52.7|           66.5|\n",
            "|       Norway|                        77.1|                      63.9|                              58.8|                           62.6|                        68.0|           64.1|\n",
            "|      Finland|                        77.0|                      56.7|                              62.2|                           66.3|                        68.3|           63.8|\n",
            "|United States|                        73.7|                      54.2|                              57.2|                           79.4|                        56.9|           59.6|\n",
            "|  Netherlands|                        73.0|                      54.5|                              59.7|                           68.5|                        58.8|           64.0|\n",
            "|    Hong Kong|                        72.7|                      62.1|                              60.3|                           53.6|                        62.0|           66.5|\n",
            "|       Taiwan|                        72.0|                      48.7|                              59.8|                           76.0|                        55.0|           62.8|\n",
            "|  South Korea|                        71.9|                      56.5|                              56.2|                           80.2|                        52.0|           57.1|\n",
            "|      Iceland|                        70.4|                      60.9|                              59.1|                           44.7|                        69.0|           64.0|\n",
            "|       Israel|                        70.0|                      47.2|                              60.3|                           77.8|                        55.5|           55.8|\n",
            "|      Austria|                        70.0|                      54.9|                              50.3|                           66.8|                        63.4|           61.0|\n",
            "|        China|                        68.8|                      54.0|                              62.5|                           72.0|                        50.3|           53.9|\n",
            "|       Canada|                        68.5|                      55.6|                              53.1|                           61.4|                        59.3|           62.6|\n",
            "|      Ireland|                        68.4|                      47.0|                              60.8|                           58.9|                        62.0|           63.1|\n",
            "|    Australia|                        67.8|                      55.2|                              53.0|                           55.3|                        63.3|           63.0|\n",
            "|      Belgium|                        67.6|                      46.8|                              55.0|                           64.9|                        57.2|           65.4|\n",
            "|      Germany|                        67.2|                      47.2|                              47.7|                           76.1|                        61.8|           55.3|\n",
            "+-------------+----------------------------+--------------------------+----------------------------------+-------------------------------+----------------------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/disease_death.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"disease_death.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "disease_death = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"disease_death.csv\")\n",
        "\n",
        "# Select specific columns\n",
        "disease_death = disease_death.select('location_name', 'year', 'val')\n",
        "\n",
        "# Rename columns\n",
        "disease_death = disease_death \\\n",
        "    .withColumnRenamed(\"location_name\", \"Entity\") \\\n",
        "    .withColumnRenamed(\"year\", \"Year\") \\\n",
        "    .withColumnRenamed(\"val\", \"disease_death\")\n",
        "\n",
        "# Replace 'United States of America' with 'United States'\n",
        "disease_death = disease_death.withColumn(\n",
        "    \"Entity\",\n",
        "    when(col(\"Entity\") == \"United States of America\", \"United States\")\n",
        "    .when(col(\"Entity\") == \"Viet Nam\", \"Vietnam\")\n",
        "    .otherwise(col(\"Entity\"))\n",
        ")\n",
        "\n",
        "# Show the result to verify\n",
        "disease_death.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UEwlGfA1-Es",
        "outputId": "9f24543a-1dd5-486d-f29d-177ffd32a3f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to disease_death.csv.\n",
            "+--------------------+----+------------------+\n",
            "|              Entity|Year|     disease_death|\n",
            "+--------------------+----+------------------+\n",
            "|         Timor-Leste|1980|  424.527743243694|\n",
            "|             Georgia|1980| 67.82102407337028|\n",
            "|             Romania|1980| 59.82198316004107|\n",
            "|             Ireland|1980|  63.9745228156746|\n",
            "|              Israel|1980|20.834051018341135|\n",
            "|            Mongolia|1980| 262.6549797011903|\n",
            "|   Republic of Korea|1980| 55.73448324410599|\n",
            "|             Vietnam|1980|127.20903743626477|\n",
            "|Saint Vincent and...|1980| 46.26843449325626|\n",
            "|           Australia|1980|13.350875120580758|\n",
            "|            Slovakia|1980|51.259039214540984|\n",
            "|              Poland|1980| 33.61888341660194|\n",
            "|          Kazakhstan|1980| 88.43572310752184|\n",
            "|            Cambodia|1980| 426.3235915819307|\n",
            "|                Oman|1980| 90.58843249813904|\n",
            "|Venezuela (Boliva...|1980|  40.3951245759084|\n",
            "|Democratic People...|1980| 92.67422355230126|\n",
            "|         Netherlands|1980|22.001170985692195|\n",
            "|              Canada|1980|18.436246037330555|\n",
            "|              Serbia|1980| 20.62464508053528|\n",
            "+--------------------+----+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/average_monthly_surface_temp.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"average_monthly_surface_temp.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "average_monthly_surface_temp = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"average_monthly_surface_temp.csv\")\n",
        "\n",
        "# Rename columns\n",
        "average_monthly_surface_temp = average_monthly_surface_temp \\\n",
        "    .withColumnRenamed(\"year\", \"Year\")\n",
        "\n",
        "# Group by 'Entity', 'Code', and 'Year' and compute the average of 'temperature_2m'\n",
        "average_yearly_temp = average_monthly_surface_temp.groupBy(\"Entity\", \"Code\", \"Year\") \\\n",
        "    .agg(avg(col(\"temperature_2m\")).alias(\"temperature_2m\"))\n",
        "\n",
        "# Show the result\n",
        "average_yearly_temp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luOTlBL62OH2",
        "outputId": "39eb78f3-e232-4d33-ab82-d33ebe25f650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to average_monthly_surface_temp.csv.\n",
            "+--------------------+----+----+------------------+\n",
            "|              Entity|Code|Year|    temperature_2m|\n",
            "+--------------------+----+----+------------------+\n",
            "|             Albania| ALB|1951|12.695347875000001|\n",
            "|             Algeria| DZA|1962|      22.094080375|\n",
            "|             Algeria| DZA|1997| 23.34587508333333|\n",
            "|              Angola| AGO|1953|21.223411083333335|\n",
            "|           Argentina| ARG|1983|      14.275761025|\n",
            "|           Argentina| ARG|2009|        15.0817606|\n",
            "|           Argentina| ARG|2018|       14.94397495|\n",
            "|          Azerbaijan| AZE|1961|11.979814019166662|\n",
            "|          Bangladesh| BGD|2003|25.181135416666667|\n",
            "|             Belarus| BLR|1992|      7.0296958625|\n",
            "|             Bolivia| BOL|1941|19.235697416666664|\n",
            "|             Bolivia| BOL|2023|21.484539916666666|\n",
            "|Bosnia and Herzeg...| BIH|1978|  8.48321621833333|\n",
            "|Bosnia and Herzeg...| BIH|1982|        9.72992165|\n",
            "|            Bulgaria| BGR|1965|10.252742091666667|\n",
            "|             Burundi| BDI|1946|        20.0153275|\n",
            "|          Cape Verde| CPV|2010| 23.66065333333334|\n",
            "|               Chile| CHL|1979| 9.103750183333332|\n",
            "|               China| CHN|1957|5.6288620400000005|\n",
            "|             Comoros| COM|1953|24.497803083333334|\n",
            "+--------------------+----+----+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "public_url = \"https://storage.googleapis.com/travel-analysis-bucket/international_tourist_trips.csv\"\n",
        "\n",
        "# Local path to save the file\n",
        "destination_file = \"international_tourist_trips.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(public_url)\n",
        "if response.status_code == 200:\n",
        "    with open(destination_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully to {destination_file}.\")\n",
        "\n",
        "# Load the CSV file\n",
        "international_tourist_trips = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"international_tourist_trips.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq0Bp-UR2U8w",
        "outputId": "ecd1c4ff-8303-41b6-a980-abd37167f723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to international_tourist_trips.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with the first join\n",
        "merge_df = international_tourist_trips.join(average_yearly_temp,\n",
        "                                            on=[\"Entity\", \"Code\", \"Year\"],\n",
        "                                            how=\"inner\")\n",
        "\n",
        "# Continue merging other DataFrames\n",
        "merge_df = merge_df.join(annual_co2, on=[\"Entity\", \"Code\", \"Year\"], how=\"inner\") \\\n",
        "                   .join(annual_ghe, on=[\"Entity\", \"Code\", \"Year\"], how=\"inner\") \\\n",
        "                   .join(energy_consumption, on=[\"Entity\", \"Code\", \"Year\"], how=\"inner\") \\\n",
        "                   .join(average_precipitation, on=[\"Entity\", \"Code\", \"Year\"], how=\"inner\") \\\n",
        "                   .join(gdp_ppp_per_capita, on=[\"Entity\", \"Code\", \"Year\"], how=\"inner\") \\\n",
        "                   .join(gdp_nominal_per_capita, on=[\"Entity\", \"Code\", \"Year\"], how=\"inner\") \\\n",
        "                   .join(inflation_rate, on=[\"Entity\", \"Code\", \"Year\"], how=\"inner\") \\\n",
        "                   .join(crime_rate, on=[\"Entity\", \"Code\", \"Year\"], how=\"inner\") \\\n",
        "                   .join(intl_tourist_spending, on=[\"Entity\", \"Code\", \"Year\"], how=\"inner\")\n",
        "\n",
        "# Merge from Wiki\n",
        "merge_df = merge_df.join(number_of_UNESCO_WHS, on=[\"Entity\"], how=\"left\") \\\n",
        "                   .join(population, on=[\"Entity\"], how=\"inner\") \\\n",
        "                   .join(political_stability, on=[\"Entity\"], how=\"inner\") \\\n",
        "                   .join(infrastructure, on=[\"Entity\"], how=\"inner\")\n",
        "\n",
        "# Merge from Google Drive upload\n",
        "merge_df = merge_df.join(disease_death, on=[\"Entity\", \"Year\"], how=\"inner\")\n",
        "\n",
        "# Calculate purchasing_power_index\n",
        "merge_df = merge_df.withColumn(\"purchasing_power_index\",\n",
        "                               col(\"ny_gdp_pcap_pp_kd\") / col(\"ny_gdp_pcap_kd\"))\n",
        "\n",
        "merge_df = merge_df.sort(\"Entity\", \"Year\")\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "merge_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUBlitrm2Y2B",
        "outputId": "30cb1d18-8c83-44f0-f95e-390545cf633f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+----+---------------------------------+------------------+-----------+-----------+-------------------------------+-------------------+-----------------+--------------+--------------+----------+--------------------------+--------------------+----------+-------------------+----------------------------+--------------------------+----------------------------------+-------------------------------+----------------------------+---------------+------------------+----------------------+\n",
            "|   Entity|Year|Code|in_tour_arrivals_ovn_vis_tourists|    temperature_2m| annual_co2| annual_ghe|primary_energy_consumption__twh|total_precipitation|ny_gdp_pcap_pp_kd|ny_gdp_pcap_kd|inflation_rate|crime_rate|outbound_exp_us_cpi_adjust|number_of_UNESCO_WHS|Population|political_stability|Overall Infrastructure Score|Basic Infrastructure Score|Technological Infrastructure Score|Scientific Infrastructure Score|Health and Environment Score|Education Score|     disease_death|purchasing_power_index|\n",
            "+---------+----+----+---------------------------------+------------------+-----------+-----------+-------------------------------+-------------------+-----------------+--------------+--------------+----------+--------------------------+--------------------+----------+-------------------+----------------------------+--------------------------+----------------------------------+-------------------------------+----------------------------+---------------+------------------+----------------------+\n",
            "|Australia|1998| AUS|                        3825000.0|          22.45062|3.3413584E8|6.8027264E8|                      1246.3304|           538.7993|        35990.387|     42966.793|     0.8601346|  1.795538|                9.857476E9|                  20|  27122411|               19.6|                        67.8|                      55.2|                              53.0|                           55.3|                        63.3|           63.0|12.996781214561302|    0.8376326108397246|\n",
            "|Australia|1999| AUS|                        4109000.0| 21.75082541666666| 3.439595E8| 6.966192E8|                      1297.3473|           524.8562|         37388.59|     44636.023|     1.4831294| 2.0477753|              1.0054714E10|                  20|  27122411|               19.6|                        67.8|                      55.2|                              53.0|                           55.3|                        63.3|           63.0|12.267747399312912|    0.8376326448259065|\n",
            "|Australia|2000| AUS|                        4530000.0|21.287555416666667|3.5000797E8| 7.450101E8|                      1305.2594|          656.68506|         38412.64|     45858.582|      4.457435| 1.9034636|              1.0052003E10|                  20|  27122411|               19.6|                        67.8|                      55.2|                              53.0|                           55.3|                        63.3|           63.0|14.060434934474385|    0.8376325286289924|\n",
            "|Australia|2001| AUS|                        4435000.0|21.458197583333334|3.5778368E8|7.2614176E8|                      1315.3853|           500.8003|         38690.57|     46190.383|     4.4071355| 1.8027713|                8.776421E9|                  20|  27122411|               19.6|                        67.8|                      55.2|                              53.0|                           55.3|                        63.3|           63.0|14.299105601979615|    0.8376325868525489|\n",
            "|Australia|2002| AUS|                        4420000.0|22.310005999999998|3.6253654E8|6.9402586E8|                       1341.745|           296.4898|         39775.27|      47485.34|     2.9815745|  1.879251|                8.982864E9|                  20|  27122411|               19.6|                        67.8|                      55.2|                              53.0|                           55.3|                        63.3|           63.0|15.480271764982058|    0.8376326251428335|\n",
            "|Australia|2003| AUS|                        4354000.0|         22.224908|3.6962966E8|7.0267757E8|                      1360.8622|          437.69666|        40535.668|     48393.137|      2.732596| 1.5330728|              1.0807367E10|                  20|  27122411|               19.6|                        67.8|                      55.2|                              53.0|                           55.3|                        63.3|           63.0| 16.54631118143097|    0.8376325758753767|\n",
            "|Australia|2004| AUS|                        4774000.0|22.078052083333336|3.8315904E8| 7.355036E8|                      1404.4441|           447.5443|         41798.72|     49901.016|     2.3432553|  1.319946|              1.5302749E10|                  20|  27122411|               19.6|                        67.8|                      55.2|                              53.0|                           55.3|                        63.3|           63.0|16.064099879405155|    0.8376326445938496|\n",
            "|Australia|2005| AUS|                        5020000.0|22.727581416666666|  3.86176E8| 6.329087E8|                      1410.3502|          354.92398|         42595.43|      50852.16|     2.6918316| 1.2839751|              1.6301892E10|                  20|  27122411|               19.6|                        67.8|                      55.2|                              53.0|                           55.3|                        63.3|           63.0|14.897947021531518|    0.8376326590650229|\n",
            "|Australia|2006| AUS|                        5064000.0| 22.04545441666667|3.9235997E8| 6.641518E8|                      1470.8378|          432.42755|        43182.387|     51552.895|     3.5552878| 1.3729398|              1.6851628E10|                  20|  27122411|               19.6|                        67.8|                      55.2|                              53.0|                           55.3|                        63.3|           63.0| 14.14245745270484|    0.8376326295545576|\n",
            "|  Austria|1995| AUT|                         1.7173E7| 6.161476087499999| 6.406075E7| 7.572559E7|                      367.66147|          1290.2373|        40425.395|     33790.484|     2.2433662| 0.9814659|              1.9359388E10|                  12|   9198124|               23.1|                        70.0|                      54.9|                              50.3|                           66.8|                        63.4|           61.0|14.142857277763952|    1.1963544233340961|\n",
            "|  Austria|1996| AUT|                          1.709E7| 5.123707641666667| 6.740524E7| 7.915825E7|                      375.69647|           1309.531|        41319.375|      34537.74|     1.8609712| 1.2440248|              1.9029213E10|                  12|   9198124|               23.1|                        70.0|                      54.9|                              50.3|                           66.8|                        63.4|           61.0| 14.81381281069943|    1.1963543358656357|\n",
            "|  Austria|1997| AUT|                         1.6647E7| 6.306686017499999| 6.730182E7| 7.939522E7|                      380.56857|          1231.8765|        42136.664|     35220.887|     1.3059785| 0.8284075|               1.698576E10|                  12|   9198124|               23.1|                        70.0|                      54.9|                              50.3|                           66.8|                        63.4|           61.0|15.055071144117797|    1.1963544245776658|\n",
            "|  Austria|1998| AUT|                         1.7352E7|6.3832149750000005| 6.693673E7| 7.950449E7|                      389.24725|          1267.0883|         43597.89|      36442.29|     0.9224672| 0.9654129|              1.5926556E10|                  12|   9198124|               23.1|                        70.0|                      54.9|                              50.3|                           66.8|                        63.4|           61.0|15.187656269644158|    1.1963542905783362|\n",
            "|  Austria|1999| AUT|                         1.7467E7| 6.363406937499999|  6.56678E7|  7.73581E7|                       392.8037|          1341.7551|        45060.617|      37664.94|    0.56899375|0.75080925|                1.09429E10|                  12|   9198124|               23.1|                        70.0|                      54.9|                              50.3|                           66.8|                        63.4|           61.0|15.888480867850976|    1.1963544081047255|\n",
            "|  Austria|2000| AUT|                         1.7982E7|     7.17581073075|  6.61771E7| 7.666366E7|                      395.17072|          1271.3025|         46469.86|      38842.89|      2.344863| 1.0236655|                9.806525E9|                  12|   9198124|               23.1|                        70.0|                      54.9|                              50.3|                           66.8|                        63.4|           61.0|15.092671318439018|    1.1963543392368592|\n",
            "|  Austria|2001| AUT|                          1.818E7| 6.357960706666666| 7.016558E7| 8.056189E7|                      406.59348|          1231.8984|        46878.918|      39184.81|     2.6500008|0.87068725|              1.0068016E10|                  12|   9198124|               23.1|                        70.0|                      54.9|                              50.3|                           66.8|                        63.4|           61.0| 13.59654185734848|    1.1963543526177618|\n",
            "|  Austria|2002| AUT|                         1.8611E7| 7.335260416666666|  7.19694E7| 8.230832E7|                      406.82816|           1339.831|        47419.277|     39636.484|     1.8103579|0.80455583|              1.0395899E10|                  12|   9198124|               23.1|                        70.0|                      54.9|                              50.3|                           66.8|                        63.4|           61.0|13.027043074635374|     1.196354272997575|\n",
            "|  Austria|2003| AUT|                         1.9078E7| 6.850630133333333| 7.739606E7|8.9567384E7|                      407.70358|          964.65704|        47633.113|     39815.223|     1.3555537| 0.6157557|              1.2701638E10|                  12|   9198124|               23.1|                        70.0|                      54.9|                              50.3|                           66.8|                        63.4|           61.0| 14.55072110166426|    1.1963542939342573|\n",
            "|  Austria|2004| AUT|                         1.9374E7| 6.349265058333334| 7.769356E7| 8.785792E7|                      422.45746|          1228.8599|        48633.273|     40651.227|      2.061206| 0.7220467|              1.3251289E10|                  12|   9198124|               23.1|                        70.0|                      54.9|                              50.3|                           66.8|                        63.4|           61.0|14.216758916979824|    1.1963543683441584|\n",
            "|  Austria|2005| AUT|                         1.9952E7| 6.035261331666665| 7.909184E7| 8.901813E7|                      431.35013|          1282.2631|        49387.027|      41281.27|     2.2991378|0.65637255|              1.2926072E10|                  12|   9198124|               23.1|                        70.0|                      54.9|                              50.3|                           66.8|                        63.4|           61.0| 14.60696223166963|    1.1963543515012984|\n",
            "+---------+----+----+---------------------------------+------------------+-----------+-----------+-------------------------------+-------------------+-----------------+--------------+--------------+----------+--------------------------+--------------------+----------+-------------------+----------------------------+--------------------------+----------------------------------+-------------------------------+----------------------------+---------------+------------------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kuwait has Nan number_of_UNESCO_WHS\n",
        "merge_df = merge_df.fillna(0)"
      ],
      "metadata": {
        "id": "TxaOka3fG04T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZh-MgQP8V_A",
        "outputId": "73837729-10ff-4e49-bba8-84982fb03f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Entity',\n",
              " 'Year',\n",
              " 'Code',\n",
              " 'in_tour_arrivals_ovn_vis_tourists',\n",
              " 'temperature_2m',\n",
              " 'annual_co2',\n",
              " 'annual_ghe',\n",
              " 'primary_energy_consumption__twh',\n",
              " 'total_precipitation',\n",
              " 'ny_gdp_pcap_pp_kd',\n",
              " 'ny_gdp_pcap_kd',\n",
              " 'inflation_rate',\n",
              " 'crime_rate',\n",
              " 'outbound_exp_us_cpi_adjust',\n",
              " 'number_of_UNESCO_WHS',\n",
              " 'Population',\n",
              " 'political_stability',\n",
              " 'Overall Infrastructure Score',\n",
              " 'Basic Infrastructure Score',\n",
              " 'Technological Infrastructure Score',\n",
              " 'Scientific Infrastructure Score',\n",
              " 'Health and Environment Score',\n",
              " 'Education Score',\n",
              " 'disease_death',\n",
              " 'purchasing_power_index']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique values in 'Entity' column\n",
        "unique_entities = merge_df.select(\"Entity\").distinct().collect()\n",
        "unique_entities = [row[\"Entity\"] for row in unique_entities]\n",
        "print(unique_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMBfEkuS8i4_",
        "outputId": "e1c62c91-e49b-410e-8b0d-409eaca9079a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sweden', 'Philippines', 'Malaysia', 'Turkey', 'Germany', 'Jordan', 'France', 'Greece', 'Slovakia', 'Belgium', 'Qatar', 'Finland', 'Ghana', 'Peru', 'India', 'China', 'United States', 'Kuwait', 'Chile', 'Croatia', 'Nigeria', 'Italy', 'Lithuania', 'Norway', 'Spain', 'Denmark', 'Ireland', 'Thailand', 'Iceland', 'Cyprus', 'Mexico', 'Estonia', 'Mongolia', 'Saudi Arabia', 'Switzerland', 'Latvia', 'United Arab Emirates', 'Canada', 'Brazil', 'Slovenia', 'Botswana', 'Luxembourg', 'New Zealand', 'Poland', 'Portugal', 'Australia', 'Romania', 'Bulgaria', 'Austria', 'Kazakhstan', 'South Africa', 'Bahrain', 'Colombia', 'Hungary', 'United Kingdom', 'Netherlands']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql.functions import corr\n",
        "\n",
        "# # Define columns of interest (excluding 'Code', 'Entity', 'Year')\n",
        "# columns_of_interest = [col for col in merge_df.columns if col not in ['Code', 'Entity', 'Year']]\n",
        "\n",
        "# # Calculate correlations for each numeric column with the target column\n",
        "# target_column = 'in_tour_arrivals_ovn_vis_tourists'\n",
        "\n",
        "# # Create a dictionary to store correlations\n",
        "# correlations = {}\n",
        "\n",
        "# for col in columns_of_interest:\n",
        "#     if col != target_column:  # Skip the target column itself\n",
        "#         correlation = merge_df.stat.corr(target_column, col)\n",
        "#         correlations[col] = correlation\n",
        "\n",
        "# # Sort correlations in descending order\n",
        "# sorted_correlations = dict(sorted(correlations.items(), key=lambda x: x[1], reverse=True))\n",
        "# sorted_correlations"
      ],
      "metadata": {
        "id": "5mtxD5jN-AaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "merge_df_1 = merge_df\n",
        "\n",
        "# Drop the 'EntityIndex' column if it exists\n",
        "if 'EntityIndex' in merge_df_1.columns:\n",
        "  merge_df_1 = merge_df_1.drop(\"EntityIndex\")\n",
        "\n",
        "# Convert 'Entity' to numerical values\n",
        "indexer = StringIndexer(inputCol=\"Entity\", outputCol=\"EntityIndex\")\n",
        "merge_df_1 = indexer.fit(merge_df_1).transform(merge_df_1)\n",
        "\n",
        "# Drop 'Code' column\n",
        "merge_df_1 = merge_df_1.drop(\"Code\")\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "if 'features' in merge_df_1.columns:\n",
        "  merge_df_1 = merge_df_1.drop(\"features\")\n",
        "feature_cols = [col for col in merge_df_1.columns if col not in ['in_tour_arrivals_ovn_vis_tourists', 'Entity', 'Year']]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "merge_df_1 = assembler.transform(merge_df_1)\n",
        "\n",
        "# Split data\n",
        "train_df, test_df = merge_df_1.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train Random Forest Regressor model\n",
        "rf = RandomForestRegressor(featuresCol=\"features\",\n",
        "                           labelCol=\"in_tour_arrivals_ovn_vis_tourists\",\n",
        "                           numTrees=100,\n",
        "                           maxBins=75,\n",
        "                           maxDepth=15,\n",
        "                           seed=42)\n",
        "rf_model = rf.fit(train_df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = rf_model.transform(test_df)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "mae = evaluator.evaluate(predictions)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "\n",
        "mse_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "mse = mse_evaluator.evaluate(predictions)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "r2_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "r2 = r2_evaluator.evaluate(predictions)\n",
        "print(\"R² Score:\", r2)\n",
        "\n",
        "# Calculate MAPE\n",
        "predictions = predictions.withColumn(\"absolute_percentage_error\",\n",
        "                                     (abs(predictions[\"in_tour_arrivals_ovn_vis_tourists\"] - predictions[\"prediction\"]) / predictions[\"in_tour_arrivals_ovn_vis_tourists\"]))\n",
        "mape = predictions.select(\"absolute_percentage_error\").agg({\"absolute_percentage_error\": \"avg\"}).collect()[0][0]\n",
        "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
        "\n",
        "# Calculate Relative MAE\n",
        "mean_y_true = test_df.select(\"in_tour_arrivals_ovn_vis_tourists\").agg({\"in_tour_arrivals_ovn_vis_tourists\": \"avg\"}).collect()[0][0]\n",
        "relative_mae = mae / mean_y_true\n",
        "print(f\"Relative Mean Absolute Error (Relative MAE): {relative_mae}\")\n",
        "\n",
        "# Show Actual vs Predicted\n",
        "rf_df = predictions.select(\"in_tour_arrivals_ovn_vis_tourists\", \"prediction\")\n",
        "rf_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBiYlPn--EqA",
        "outputId": "130d803b-c6fc-48c5-a68e-aa0241de7f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 2063952.4489734464\n",
            "Mean Squared Error (MSE): 24742653528338.79\n",
            "R² Score: 0.9209741597837229\n",
            "Mean Absolute Percentage Error (MAPE): 0.2760789750219505\n",
            "Relative Mean Absolute Error (Relative MAE): 0.1505354249759593\n",
            "+---------------------------------+------------------+\n",
            "|in_tour_arrivals_ovn_vis_tourists|        prediction|\n",
            "+---------------------------------+------------------+\n",
            "|                        4530000.0| 4420167.757936507|\n",
            "|                        4774000.0| 5061359.285714285|\n",
            "|                        5064000.0| 8111485.445714286|\n",
            "|                         1.7467E7|       1.9919056E7|\n",
            "|                         1.9952E7|      1.99558425E7|\n",
            "|                         2.1355E7|       2.2334452E7|\n",
            "|                         2.6728E7|        2.551664E7|\n",
            "|                         1.2728E7|       2.0531084E7|\n",
            "|                        4366000.0| 3952991.333333334|\n",
            "|                        3849000.0|  3935230.61904762|\n",
            "|                         827000.0|         3130456.0|\n",
            "|                        5829000.0| 5548316.547619048|\n",
            "|                        6457000.0| 6081342.445054946|\n",
            "|                        1193000.0| 1284302.060264297|\n",
            "|                        2101000.0|1495725.8181818181|\n",
            "|                        2850000.0| 4368620.342394534|\n",
            "|                        5358000.0| 4673007.404486038|\n",
            "|                        3186000.0| 3391456.090909091|\n",
            "|                        6898000.0|         5985675.0|\n",
            "|                        8883000.0| 5494370.704906205|\n",
            "+---------------------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Relative MAE\n",
        "mean_y_true = test_df.select(\"in_tour_arrivals_ovn_vis_tourists\").agg({\"in_tour_arrivals_ovn_vis_tourists\": \"avg\"}).collect()[0][0]\n",
        "relative_mae = mae / mean_y_true\n",
        "print(f\"Relative Mean Absolute Error (Relative MAE): {relative_mae}\")\n",
        "\n",
        "# Show Actual vs Predicted\n",
        "rf_df = predictions.select(\"in_tour_arrivals_ovn_vis_tourists\", \"prediction\")\n",
        "rf_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlvqiMjK-I72",
        "outputId": "974a1d60-1e6e-4fd4-b1f4-21df574e7d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relative Mean Absolute Error (Relative MAE): 0.1505354249759593\n",
            "+---------------------------------+------------------+\n",
            "|in_tour_arrivals_ovn_vis_tourists|        prediction|\n",
            "+---------------------------------+------------------+\n",
            "|                        4530000.0| 4420167.757936507|\n",
            "|                        4774000.0| 5061359.285714285|\n",
            "|                        5064000.0| 8111485.445714286|\n",
            "|                         1.7467E7|       1.9919056E7|\n",
            "|                         1.9952E7|      1.99558425E7|\n",
            "|                         2.1355E7|       2.2334452E7|\n",
            "|                         2.6728E7|        2.551664E7|\n",
            "|                         1.2728E7|       2.0531084E7|\n",
            "|                        4366000.0| 3952991.333333334|\n",
            "|                        3849000.0|  3935230.61904762|\n",
            "|                         827000.0|         3130456.0|\n",
            "|                        5829000.0| 5548316.547619048|\n",
            "|                        6457000.0| 6081342.445054946|\n",
            "|                        1193000.0| 1284302.060264297|\n",
            "|                        2101000.0|1495725.8181818181|\n",
            "|                        2850000.0| 4368620.342394534|\n",
            "|                        5358000.0| 4673007.404486038|\n",
            "|                        3186000.0| 3391456.090909091|\n",
            "|                        6898000.0|         5985675.0|\n",
            "|                        8883000.0| 5494370.704906205|\n",
            "+---------------------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# from pyspark.ml.feature import VectorAssembler\n",
        "# from pyspark.ml.regression import RandomForestRegressor\n",
        "# from pyspark.ml.evaluation import RegressionEvaluator\n",
        "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "# import pandas as pd\n",
        "\n",
        "# # Initialize the RandomForestRegressor\n",
        "# rf = RandomForestRegressor(featuresCol=\"features\", labelCol='in_tour_arrivals_ovn_vis_tourists', seed=42)\n",
        "\n",
        "# # Sequential Feature Selection (custom implementation)\n",
        "# selected_features = []\n",
        "# remaining_features = feature_cols.copy()\n",
        "# n_features_to_select = 10  # Number of features to select\n",
        "\n",
        "# for _ in range(n_features_to_select):\n",
        "#     best_feature = None\n",
        "#     best_score = float(\"-inf\")\n",
        "\n",
        "#     for feature in remaining_features:\n",
        "#         current_features = selected_features + [feature]\n",
        "\n",
        "#         # Assemble current features\n",
        "#         assembler = VectorAssembler(inputCols=current_features, outputCol=\"features\")\n",
        "#         train_df = assembler.transform(train_df).select(\"features\", label_column)\n",
        "\n",
        "#         # Cross-validation\n",
        "#         param_grid = ParamGridBuilder().build()\n",
        "#         evaluator = RegressionEvaluator(labelCol=label_column, metricName=\"r2\")\n",
        "#         crossval = CrossValidator(estimator=rf, estimatorParamMaps=param_grid,\n",
        "#                                   evaluator=evaluator, numFolds=5)\n",
        "\n",
        "#         cv_model = crossval.fit(train_df)\n",
        "#         score = max(cv_model.avgMetrics)\n",
        "\n",
        "#         # Check if the current feature is better\n",
        "#         if score > best_score:\n",
        "#             best_score = score\n",
        "#             best_feature = feature\n",
        "\n",
        "#     # Update selected and remaining features\n",
        "#     if best_feature:\n",
        "#         selected_features.append(best_feature)\n",
        "#         remaining_features.remove(best_feature)\n",
        "\n",
        "# print(\"Selected Features:\", selected_features)\n",
        "\n",
        "# # Final Training and Evaluation\n",
        "# assembler = VectorAssembler(inputCols=selected_features, outputCol=\"features\")\n",
        "# train_df = assembler.transform(train_df).select(\"features\", label_column)\n",
        "# test_df = assembler.transform(test_df).select(\"features\", label_column)\n",
        "\n",
        "# final_model = rf.fit(train_df)\n",
        "# predictions = final_model.transform(test_df)\n",
        "\n",
        "# # Evaluate the model\n",
        "# evaluator = RegressionEvaluator(labelCol=label_column, metricName=\"r2\")\n",
        "# r2_score = evaluator.evaluate(predictions)\n",
        "# print(\"R² Score with selected features:\", r2_score)"
      ],
      "metadata": {
        "id": "ZYK0YBQ8Mybe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "merge_df_4 = merge_df\n",
        "\n",
        "# Drop 'EntityIndex' column if it exists\n",
        "if 'EntityIndex' in merge_df_4.columns:\n",
        "  merge_df_4 = merge_df_4.drop(\"EntityIndex\")\n",
        "\n",
        "# Convert 'Entity' to numerical valeus\n",
        "indexer = StringIndexer(inputCol=\"Entity\", outputCol=\"EntityIndex\")\n",
        "merge_df_4 = indexer.fit(merge_df_4).transform(merge_df_4)\n",
        "\n",
        "# Drop 'Code' column\n",
        "merge_df_4 = merge_df_1.drop(\"Code\")\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "if 'features' in merge_df_4.columns:\n",
        "  merge_df_4 = merge_df_4.drop(\"features\")\n",
        "feature_cols = [col for col in merge_df_4.columns if col not in ['in_tour_arrivals_ovn_vis_tourists', 'Entity', 'Year']]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "merge_df_4 = assembler.transform(merge_df_4)\n",
        "\n",
        "# Split data\n",
        "train_df, test_df = merge_df_4.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf = RandomForestRegressor(featuresCol=\"features\",\n",
        "                           labelCol=\"in_tour_arrivals_ovn_vis_tourists\",\n",
        "                           numTrees=100,\n",
        "                           maxBins=75,\n",
        "                           maxDepth=15,\n",
        "                           seed=42)\n",
        "rf_model = rf.fit(train_df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = rf_model.transform(test_df)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "mae = evaluator.evaluate(predictions)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "\n",
        "mse_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "mse = mse_evaluator.evaluate(predictions)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "r2_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "r2 = r2_evaluator.evaluate(predictions)\n",
        "print(\"R² Score:\", r2)\n",
        "\n",
        "# Calculate MAPE\n",
        "predictions = predictions.withColumn(\"absolute_percentage_error\",\n",
        "                                     (abs(predictions[\"in_tour_arrivals_ovn_vis_tourists\"] - predictions[\"prediction\"]) / predictions[\"in_tour_arrivals_ovn_vis_tourists\"]))\n",
        "mape = predictions.select(\"absolute_percentage_error\").agg({\"absolute_percentage_error\": \"avg\"}).collect()[0][0]\n",
        "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
        "\n",
        "# Calculate Relative MAE\n",
        "mean_y_true = test_df.select(\"in_tour_arrivals_ovn_vis_tourists\").agg({\"in_tour_arrivals_ovn_vis_tourists\": \"avg\"}).collect()[0][0]\n",
        "relative_mae = mae / mean_y_true\n",
        "print(f\"Relative Mean Absolute Error (Relative MAE): {relative_mae}\")\n",
        "\n",
        "# Show Actual vs Predicted\n",
        "rf_df = predictions.select(\"in_tour_arrivals_ovn_vis_tourists\", \"prediction\")\n",
        "rf_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HiKBL3NuFsg",
        "outputId": "b3798c2d-3ccf-4562-8bdd-163855afbae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 2063952.4489734464\n",
            "Mean Squared Error (MSE): 24742653528338.79\n",
            "R² Score: 0.9209741597837229\n",
            "Mean Absolute Percentage Error (MAPE): 0.2760789750219505\n",
            "Relative Mean Absolute Error (Relative MAE): 0.1505354249759593\n",
            "+---------------------------------+------------------+\n",
            "|in_tour_arrivals_ovn_vis_tourists|        prediction|\n",
            "+---------------------------------+------------------+\n",
            "|                        4530000.0| 4420167.757936507|\n",
            "|                        4774000.0| 5061359.285714285|\n",
            "|                        5064000.0| 8111485.445714286|\n",
            "|                         1.7467E7|       1.9919056E7|\n",
            "|                         1.9952E7|      1.99558425E7|\n",
            "|                         2.1355E7|       2.2334452E7|\n",
            "|                         2.6728E7|        2.551664E7|\n",
            "|                         1.2728E7|       2.0531084E7|\n",
            "|                        4366000.0| 3952991.333333334|\n",
            "|                        3849000.0|  3935230.61904762|\n",
            "|                         827000.0|         3130456.0|\n",
            "|                        5829000.0| 5548316.547619048|\n",
            "|                        6457000.0| 6081342.445054946|\n",
            "|                        1193000.0| 1284302.060264297|\n",
            "|                        2101000.0|1495725.8181818181|\n",
            "|                        2850000.0| 4368620.342394534|\n",
            "|                        5358000.0| 4673007.404486038|\n",
            "|                        3186000.0| 3391456.090909091|\n",
            "|                        6898000.0|         5985675.0|\n",
            "|                        8883000.0| 5494370.704906205|\n",
            "+---------------------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "def spark_train_test_split(spark_df, test_size=0.2, seed=42):\n",
        "    # Convert Spark DataFrame to Pandas DataFrame\n",
        "    pandas_df = spark_df.toPandas()\n",
        "\n",
        "    # Perform train-test split using sklearn\n",
        "    train_df, test_df = train_test_split(pandas_df, test_size=test_size, random_state=seed)\n",
        "\n",
        "    # Convert Pandas DataFrame back to Spark DataFrame\n",
        "    train_spark_df = spark.createDataFrame(train_df)\n",
        "    test_spark_df = spark.createDataFrame(test_df)\n",
        "\n",
        "    return train_spark_df, test_spark_df\n",
        "\n",
        "merge_df_4 = merge_df\n",
        "\n",
        "# Drop 'EntityIndex' column if it exists\n",
        "if 'EntityIndex' in merge_df_4.columns:\n",
        "  merge_df_4 = merge_df_4.drop(\"EntityIndex\")\n",
        "\n",
        "# Convert 'Entity' to numerical values\n",
        "indexer = StringIndexer(inputCol=\"Entity\", outputCol=\"EntityIndex\")\n",
        "merge_df_4 = indexer.fit(merge_df_4).transform(merge_df_4)\n",
        "\n",
        "# Drop 'Code' column\n",
        "merge_df_4 = merge_df_4.drop(\"Code\")\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "if 'features' in merge_df_4.columns:\n",
        "  merge_df_4 = merge_df_4.drop(\"features\")\n",
        "feature_cols = [col for col in merge_df_4.columns if col not in ['in_tour_arrivals_ovn_vis_tourists', 'Entity', 'Year']]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "merge_df_4 = assembler.transform(merge_df_4)\n",
        "\n",
        "# Split data (80/20)\n",
        "train_df, test_df = spark_train_test_split(merge_df_4, test_size=0.2, seed=42)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf = RandomForestRegressor(featuresCol=\"features\",\n",
        "                           labelCol=\"in_tour_arrivals_ovn_vis_tourists\",\n",
        "                           numTrees=100,\n",
        "                           maxBins=95,\n",
        "                           maxDepth=15,\n",
        "                           seed=42)\n",
        "rf_model = rf.fit(train_df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = rf_model.transform(test_df)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "mae = evaluator.evaluate(predictions)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "\n",
        "mse_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "mse = mse_evaluator.evaluate(predictions)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "r2_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "r2 = r2_evaluator.evaluate(predictions)\n",
        "print(\"R² Score:\", r2)\n",
        "\n",
        "# Calculate MAPE\n",
        "predictions = predictions.withColumn(\"absolute_percentage_error\",\n",
        "                                     (abs(predictions[\"in_tour_arrivals_ovn_vis_tourists\"] - predictions[\"prediction\"]) / predictions[\"in_tour_arrivals_ovn_vis_tourists\"]))\n",
        "mape = predictions.select(\"absolute_percentage_error\").agg({\"absolute_percentage_error\": \"avg\"}).collect()[0][0]\n",
        "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
        "\n",
        "# Calculate Relative MAE\n",
        "mean_y_true = test_df.select(\"in_tour_arrivals_ovn_vis_tourists\").agg({\"in_tour_arrivals_ovn_vis_tourists\": \"avg\"}).collect()[0][0]\n",
        "relative_mae = mae / mean_y_true\n",
        "print(f\"Relative Mean Absolute Error (Relative MAE): {relative_mae}\")\n",
        "\n",
        "# Show Actual vs Predicted\n",
        "rf_df = predictions.select(\"in_tour_arrivals_ovn_vis_tourists\", \"prediction\")\n",
        "rf_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKAewO4MTVvN",
        "outputId": "caedc9db-3af5-488c-9e9c-050e7f56c86a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 1558738.0949321734\n",
            "Mean Squared Error (MSE): 9456124279577.775\n",
            "R² Score: 0.9763371398122825\n",
            "Mean Absolute Percentage Error (MAPE): 0.19921656298505802\n",
            "Relative Mean Absolute Error (Relative MAE): 0.11099814355930294\n",
            "+---------------------------------+--------------------+\n",
            "|in_tour_arrivals_ovn_vis_tourists|          prediction|\n",
            "+---------------------------------+--------------------+\n",
            "|                         1.0951E7|           9601024.0|\n",
            "|                         945000.0|   950112.1205533595|\n",
            "|                        4150000.0|  3704385.9836996333|\n",
            "|                        2100000.0|  2266973.2785547785|\n",
            "|                        8534000.0|   9448851.904761905|\n",
            "|                         2.1355E7|2.1593134285714284E7|\n",
            "|                         1.9392E7|2.1052959333333332E7|\n",
            "|                        7550000.0|           7755801.5|\n",
            "|                         1.0003E7|   9443244.913797313|\n",
            "|                        4226000.0|   3544673.305456526|\n",
            "|                        3554000.0|  4053439.2406204897|\n",
            "|                        3761000.0|   3945940.723778999|\n",
            "|                        5780000.0|   5398618.214285715|\n",
            "|                        7616000.0|   6773621.134502923|\n",
            "|                        4499000.0|   5312557.771428571|\n",
            "|                         4.6489E7|       4.978908656E7|\n",
            "|                        7311000.0|           7191812.5|\n",
            "|                        3744000.0|   3636560.936507936|\n",
            "|                          2.228E7|       3.918539414E7|\n",
            "|                        5545000.0|1.0169323285714285E7|\n",
            "+---------------------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "def spark_train_test_split(spark_df, test_size=0.2, seed=42):\n",
        "    # Convert Spark DataFrame to Pandas DataFrame\n",
        "    pandas_df = spark_df.toPandas()\n",
        "\n",
        "    # Perform train-test split using sklearn\n",
        "    train_df, test_df = train_test_split(pandas_df, test_size=test_size, random_state=seed)\n",
        "\n",
        "    # Convert Pandas DataFrame back to Spark DataFrame\n",
        "    train_spark_df = spark.createDataFrame(train_df)\n",
        "    test_spark_df = spark.createDataFrame(test_df)\n",
        "\n",
        "    return train_spark_df, test_spark_df\n",
        "\n",
        "merge_df_4 = merge_df\n",
        "\n",
        "# Drop 'EntityIndex' column if it exists\n",
        "if 'EntityIndex' in merge_df_4.columns:\n",
        "  merge_df_4 = merge_df_4.drop(\"EntityIndex\")\n",
        "\n",
        "# Convert 'Entity' to numerical index\n",
        "indexer = StringIndexer(inputCol=\"Entity\", outputCol=\"EntityIndex\")\n",
        "merge_df_4 = indexer.fit(merge_df_4).transform(merge_df_4)\n",
        "\n",
        "# Drop 'Code' column\n",
        "merge_df_4 = merge_df_4.drop(\"Code\")\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "if 'features' in merge_df_4.columns:\n",
        "  merge_df_4 = merge_df_4.drop(\"features\")\n",
        "feature_cols = ['EntityIndex', 'Year', 'Population', 'political_stability',\n",
        "       'Overall Infrastructure Score', 'Basic Infrastructure Score',\n",
        "       'Technological Infrastructure Score', 'Scientific Infrastructure Score',\n",
        "       'Health and Environment Score', 'Education Score']\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "merge_df_4 = assembler.transform(merge_df_4)\n",
        "\n",
        "# Split data (80/20)\n",
        "train_df, test_df = spark_train_test_split(merge_df_4, test_size=0.2, seed=42)\n",
        "\n",
        "# Train Random Forest Regressor model\n",
        "rf = RandomForestRegressor(featuresCol=\"features\",\n",
        "                           labelCol=\"in_tour_arrivals_ovn_vis_tourists\",\n",
        "                           numTrees=100,\n",
        "                           maxBins=95,\n",
        "                           maxDepth=15,\n",
        "                           seed=42)\n",
        "rf_model = rf.fit(train_df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = rf_model.transform(test_df)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "mae = evaluator.evaluate(predictions)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "\n",
        "mse_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "mse = mse_evaluator.evaluate(predictions)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "r2_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "r2 = r2_evaluator.evaluate(predictions)\n",
        "print(\"R² Score:\", r2)\n",
        "\n",
        "# Calculate MAPE\n",
        "predictions = predictions.withColumn(\"absolute_percentage_error\",\n",
        "                                     (abs(predictions[\"in_tour_arrivals_ovn_vis_tourists\"] - predictions[\"prediction\"]) / predictions[\"in_tour_arrivals_ovn_vis_tourists\"]))\n",
        "mape = predictions.select(\"absolute_percentage_error\").agg({\"absolute_percentage_error\": \"avg\"}).collect()[0][0]\n",
        "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
        "\n",
        "# Calculate Relative MAE\n",
        "mean_y_true = test_df.select(\"in_tour_arrivals_ovn_vis_tourists\").agg({\"in_tour_arrivals_ovn_vis_tourists\": \"avg\"}).collect()[0][0]\n",
        "relative_mae = mae / mean_y_true\n",
        "print(f\"Relative Mean Absolute Error (Relative MAE): {relative_mae}\")\n",
        "\n",
        "# Show Actual vs Predicted\n",
        "rf_df = predictions.select(\"in_tour_arrivals_ovn_vis_tourists\", \"prediction\")\n",
        "rf_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxaSkf3xTDx2",
        "outputId": "a88ecfc4-b5f1-4deb-8e5d-277b8f771ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 2093630.019963216\n",
            "Mean Squared Error (MSE): 16406271551943.719\n",
            "R² Score: 0.9589451980052968\n",
            "Mean Absolute Percentage Error (MAPE): 0.24580317767295257\n",
            "Relative Mean Absolute Error (Relative MAE): 0.1490879361141523\n",
            "+---------------------------------+--------------------+\n",
            "|in_tour_arrivals_ovn_vis_tourists|          prediction|\n",
            "+---------------------------------+--------------------+\n",
            "|                         1.0951E7|    8977828.90276393|\n",
            "|                         945000.0|   904600.2345871736|\n",
            "|                        4150000.0|  3705018.3100567665|\n",
            "|                        2100000.0|  2330004.6864240174|\n",
            "|                        8534000.0|    8758691.22082235|\n",
            "|                         2.1355E7|2.1624691200440817E7|\n",
            "|                         1.9392E7|2.3392827878862794E7|\n",
            "|                        7550000.0|   8673371.370712645|\n",
            "|                         1.0003E7|1.0153239886834199E7|\n",
            "|                        4226000.0|  3075177.6343702744|\n",
            "|                        3554000.0|  3642464.6369897206|\n",
            "|                        3761000.0|  3559739.7748974804|\n",
            "|                        5780000.0|   5646007.644030038|\n",
            "|                        7616000.0|  6351674.4781185575|\n",
            "|                        4499000.0|   6870402.018931822|\n",
            "|                         4.6489E7| 4.949995124010107E7|\n",
            "|                        7311000.0|   6041646.383387906|\n",
            "|                        3744000.0|  3186688.1883116877|\n",
            "|                          2.228E7| 3.546475654376311E7|\n",
            "|                        5545000.0|1.1672107711519634E7|\n",
            "+---------------------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MxByW7G7f9e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sxZSDyzPUk85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Below are experimenting to find the ebst hyperparameters for the models"
      ],
      "metadata": {
        "id": "2i645UmTdlAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import abs, col\n",
        "\n",
        "merge_df_4 = merge_df\n",
        "\n",
        "# Drop 'EntityIndex' column if it exists\n",
        "if 'EntityIndex' in merge_df_4.columns:\n",
        "    merge_df_4 = merge_df_4.drop(\"EntityIndex\")\n",
        "\n",
        "# Convert 'Entity' to numerical index\n",
        "indexer = StringIndexer(inputCol=\"Entity\", outputCol=\"EntityIndex\")\n",
        "merge_df_4 = indexer.fit(merge_df_4).transform(merge_df_4)\n",
        "\n",
        "# Drop 'Code' column\n",
        "merge_df_4 = merge_df_4.drop(\"Code\")\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "if 'features' in merge_df_4.columns:\n",
        "    merge_df_4 = merge_df_4.drop(\"features\")\n",
        "feature_cols = [col for col in merge_df_4.columns if col not in ['in_tour_arrivals_ovn_vis_tourists', 'Entity', 'Year']]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "merge_df_4 = assembler.transform(merge_df_4)\n",
        "\n",
        "# Split data (80/20)\n",
        "train_df, test_df = merge_df_4.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Initialize lists to store results\n",
        "results = []\n",
        "\n",
        "# Testing maxBins from 60 to 125\n",
        "for maxBins in range(60, 130, 5):\n",
        "    print(f\"Testing maxBins = {maxBins}\")\n",
        "\n",
        "    # Train Random Forest Regressor\n",
        "    rf = RandomForestRegressor(featuresCol=\"features\",\n",
        "                               labelCol=\"in_tour_arrivals_ovn_vis_tourists\",\n",
        "                               numTrees=100,\n",
        "                               maxBins=maxBins,\n",
        "                               maxDepth=30,\n",
        "                               seed=42)\n",
        "    rf_model = rf.fit(train_df)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = rf_model.transform(test_df)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "    mae = evaluator.evaluate(predictions)\n",
        "\n",
        "    mse_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "    mse = mse_evaluator.evaluate(predictions)\n",
        "\n",
        "    r2_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "    r2 = r2_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Calculate MAPE\n",
        "    predictions = predictions.withColumn(\"absolute_percentage_error\",\n",
        "                                         (abs(col(\"in_tour_arrivals_ovn_vis_tourists\") - col(\"prediction\")) / col(\"in_tour_arrivals_ovn_vis_tourists\")))\n",
        "    mape = predictions.select(\"absolute_percentage_error\").agg({\"absolute_percentage_error\": \"avg\"}).collect()[0][0]\n",
        "\n",
        "    # Calculate Relative MAE\n",
        "    mean_y_true = test_df.select(\"in_tour_arrivals_ovn_vis_tourists\").agg({\"in_tour_arrivals_ovn_vis_tourists\": \"avg\"}).collect()[0][0]\n",
        "    relative_mae = mae / mean_y_true\n",
        "\n",
        "    # Store results\n",
        "    results.append((maxBins, mae, mse, r2, mape, relative_mae))\n",
        "\n",
        "    # Print results for the current maxBins\n",
        "    print(f\"maxBins: {maxBins}, MAE: {mae}, MSE: {mse}, R²: {r2}, MAPE: {mape}, Relative MAE: {relative_mae}\")\n",
        "\n",
        "# Convert results to a DataFrame for easy viewing\n",
        "results_df = spark.createDataFrame(results, [\"maxBins\", \"MAE\", \"MSE\", \"R2\", \"MAPE\", \"Relative_MAE\"])\n",
        "results_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTLWQIHZuPxD",
        "outputId": "f9f36a97-e480-43b1-fb52-bca502be2c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing maxBins = 60\n",
            "maxBins: 60, MAE: 2182460.4223137246, MSE: 30435735539078.305, R²: 0.9027909609282091, MAPE: 0.2685620605461778, Relative MAE: 0.1591788644789824\n",
            "Testing maxBins = 65\n",
            "maxBins: 65, MAE: 2192593.930424766, MSE: 27867105246663.67, R²: 0.9109949381948574, MAPE: 0.27892057026165196, Relative MAE: 0.15991795706357742\n",
            "Testing maxBins = 70\n",
            "maxBins: 70, MAE: 2173167.570738094, MSE: 28210107581454.98, R²: 0.9098994191684934, MAPE: 0.2845358950868832, Relative MAE: 0.1585010856077338\n",
            "Testing maxBins = 75\n",
            "maxBins: 75, MAE: 2064141.0139733886, MSE: 24743788901058.688, R²: 0.9209705335039851, MAPE: 0.2756340392727243, Relative MAE: 0.15054917806044338\n",
            "Testing maxBins = 80\n",
            "maxBins: 80, MAE: 2170351.6878431374, MSE: 28277270108792.72, R²: 0.9096849080147954, MAPE: 0.2739407808565266, Relative MAE: 0.15829570775201543\n",
            "Testing maxBins = 85\n",
            "maxBins: 85, MAE: 2190504.3182492987, MSE: 29102085342442.98, R²: 0.9070505213356245, MAPE: 0.28665675262885193, Relative MAE: 0.15976555013335703\n",
            "Testing maxBins = 90\n",
            "maxBins: 90, MAE: 2069706.0142175532, MSE: 26999781951402.62, R²: 0.9137650918515237, MAPE: 0.2779500713575537, Relative MAE: 0.1509550641927345\n",
            "Testing maxBins = 95\n",
            "maxBins: 95, MAE: 2147415.3477450977, MSE: 27969202112084.594, R²: 0.9106688498646751, MAPE: 0.2828232513309103, Relative MAE: 0.15662283408393804\n",
            "Testing maxBins = 100\n",
            "maxBins: 100, MAE: 2118314.2037005913, MSE: 26710584067632.51, R²: 0.9146887642348244, MAPE: 0.27656607818611684, Relative MAE: 0.15450032729449856\n",
            "Testing maxBins = 105\n",
            "maxBins: 105, MAE: 2220201.9969444447, MSE: 28468898645537.98, R²: 0.9090728634695988, MAPE: 0.2713871340853737, Relative MAE: 0.16193156548191645\n",
            "Testing maxBins = 110\n",
            "maxBins: 110, MAE: 2226808.4681055807, MSE: 29515007214159.652, R²: 0.9057316855115398, MAPE: 0.288727124258769, Relative MAE: 0.16241341182693647\n",
            "Testing maxBins = 115\n",
            "maxBins: 115, MAE: 2177243.830457516, MSE: 28462453004182.312, R²: 0.9090934502762353, MAPE: 0.2792777347614835, Relative MAE: 0.15879838969023857\n",
            "Testing maxBins = 120\n",
            "maxBins: 120, MAE: 2123500.636440242, MSE: 27428308587982.05, R²: 0.9123964157929128, MAPE: 0.27661970694922966, Relative MAE: 0.1548786024126879\n",
            "Testing maxBins = 125\n",
            "maxBins: 125, MAE: 2180401.7722203545, MSE: 28371320830232.363, R²: 0.9093845183546403, MAPE: 0.2821009568554989, Relative MAE: 0.15902871578401787\n",
            "+-------+------------------+--------------------+------------------+-------------------+-------------------+\n",
            "|maxBins|               MAE|                 MSE|                R2|               MAPE|       Relative_MAE|\n",
            "+-------+------------------+--------------------+------------------+-------------------+-------------------+\n",
            "|     60|2182460.4223137246|3.043573553907830...|0.9027909609282091| 0.2685620605461778| 0.1591788644789824|\n",
            "|     65| 2192593.930424766|2.786710524666367E13|0.9109949381948574|0.27892057026165196|0.15991795706357742|\n",
            "|     70| 2173167.570738094|2.821010758145498E13|0.9098994191684934| 0.2845358950868832| 0.1585010856077338|\n",
            "|     75|2064141.0139733886|2.474378890105868...|0.9209705335039851| 0.2756340392727243|0.15054917806044338|\n",
            "|     80|2170351.6878431374|2.827727010879272E13|0.9096849080147954| 0.2739407808565266|0.15829570775201543|\n",
            "|     85|2190504.3182492987|2.910208534244298E13|0.9070505213356245|0.28665675262885193|0.15976555013335703|\n",
            "|     90|2069706.0142175532|2.699978195140262E13|0.9137650918515237| 0.2779500713575537| 0.1509550641927345|\n",
            "|     95|2147415.3477450977|2.796920211208459...|0.9106688498646751| 0.2828232513309103|0.15662283408393804|\n",
            "|    100|2118314.2037005913|2.671058406763251E13|0.9146887642348244|0.27656607818611684|0.15450032729449856|\n",
            "|    105|2220201.9969444447|2.846889864553798E13|0.9090728634695988| 0.2713871340853737|0.16193156548191645|\n",
            "|    110|2226808.4681055807|2.951500721415965...|0.9057316855115398|  0.288727124258769|0.16241341182693647|\n",
            "|    115| 2177243.830457516|2.846245300418231...|0.9090934502762353| 0.2792777347614835|0.15879838969023857|\n",
            "|    120| 2123500.636440242|2.742830858798205E13|0.9123964157929128|0.27661970694922966| 0.1548786024126879|\n",
            "|    125|2180401.7722203545|2.837132083023236...|0.9093845183546403| 0.2821009568554989|0.15902871578401787|\n",
            "+-------+------------------+--------------------+------------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import abs, col\n",
        "\n",
        "merge_df_4 = merge_df\n",
        "\n",
        "# Drop 'EntityIndex' column if it exists\n",
        "if 'EntityIndex' in merge_df_4.columns:\n",
        "    merge_df_4 = merge_df_4.drop(\"EntityIndex\")\n",
        "\n",
        "# Convert 'Entity' to numerical index\n",
        "indexer = StringIndexer(inputCol=\"Entity\", outputCol=\"EntityIndex\")\n",
        "merge_df_4 = indexer.fit(merge_df_4).transform(merge_df_4)\n",
        "\n",
        "# Drop 'Code' column\n",
        "merge_df_4 = merge_df_4.drop(\"Code\")\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "if 'features' in merge_df_4.columns:\n",
        "    merge_df_4 = merge_df_4.drop(\"features\")\n",
        "feature_cols = [col for col in merge_df_4.columns if col not in ['in_tour_arrivals_ovn_vis_tourists', 'Entity', 'Year']]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "merge_df_4 = assembler.transform(merge_df_4)\n",
        "\n",
        "# Split data into training and testing sets (80/20)\n",
        "train_df, test_df = merge_df_4.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Initialize lists to store results\n",
        "results = []\n",
        "\n",
        " # Testing maxDepth from 10 to 30\n",
        "for maxDepth in range(10, 32, 2):\n",
        "    print(f\"Testing maxBins = {maxBins}, maxDepth = {maxDepth}\")\n",
        "\n",
        "    # Train Random Forest Regressor\n",
        "    rf = RandomForestRegressor(featuresCol=\"features\",\n",
        "                                labelCol=\"in_tour_arrivals_ovn_vis_tourists\",\n",
        "                                numTrees=100,\n",
        "                                maxBins=75,\n",
        "                                maxDepth=maxDepth,\n",
        "                                seed=42)\n",
        "    rf_model = rf.fit(train_df)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = rf_model.transform(test_df)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "    mae = evaluator.evaluate(predictions)\n",
        "\n",
        "    mse_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "    mse = mse_evaluator.evaluate(predictions)\n",
        "\n",
        "    r2_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "    r2 = r2_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Calculate MAPE\n",
        "    predictions = predictions.withColumn(\"absolute_percentage_error\",\n",
        "                                          (abs(col(\"in_tour_arrivals_ovn_vis_tourists\") - col(\"prediction\")) / col(\"in_tour_arrivals_ovn_vis_tourists\")))\n",
        "    mape = predictions.select(\"absolute_percentage_error\").agg({\"absolute_percentage_error\": \"avg\"}).collect()[0][0]\n",
        "\n",
        "    # Calculate Relative MAE\n",
        "    mean_y_true = test_df.select(\"in_tour_arrivals_ovn_vis_tourists\").agg({\"in_tour_arrivals_ovn_vis_tourists\": \"avg\"}).collect()[0][0]\n",
        "    relative_mae = mae / mean_y_true\n",
        "\n",
        "    # Store results\n",
        "    results.append((maxBins, maxDepth, mae, mse, r2, mape, relative_mae))\n",
        "\n",
        "    # Print results for the current maxBins and maxDepth\n",
        "    print(f\"maxBins: {maxBins}, maxDepth: {maxDepth}, MAE: {mae}, MSE: {mse}, R²: {r2}, MAPE: {mape}, Relative MAE: {relative_mae}\")\n",
        "\n",
        "# Convert results to a DataFrame for easy viewing\n",
        "results_df = spark.createDataFrame(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y638Gv09uRHY",
        "outputId": "a1cc0e17-311d-4233-d0c6-f6e4540c1ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing maxBins = 125, maxDepth = 10\n",
            "maxBins: 125, maxDepth: 10, MAE: 2127980.429927452, MSE: 25035634149374.344, R²: 0.9200384056731985, MAPE: 0.2981064165271906, Relative MAE: 0.15520533843644516\n",
            "Testing maxBins = 125, maxDepth = 12\n",
            "maxBins: 125, maxDepth: 12, MAE: 2089482.6619634079, MSE: 24837059311744.746, R²: 0.9206726360871481, MAPE: 0.2833115951166398, Relative MAE: 0.15239748408690543\n",
            "Testing maxBins = 125, maxDepth = 14\n",
            "maxBins: 125, maxDepth: 14, MAE: 2066832.5588562565, MSE: 24755122429257.824, R²: 0.9209343352204208, MAPE: 0.27672420027890504, Relative MAE: 0.15074548725980788\n",
            "Testing maxBins = 125, maxDepth = 16\n",
            "maxBins: 125, maxDepth: 16, MAE: 2064135.415693448, MSE: 24744577923938.312, R²: 0.9209680134349092, MAPE: 0.2757391587579784, Relative MAE: 0.15054876974703943\n",
            "Testing maxBins = 125, maxDepth = 18\n",
            "maxBins: 125, maxDepth: 18, MAE: 2063961.6279545864, MSE: 24742962666031.863, R²: 0.9209731724253577, MAPE: 0.27557160411135617, Relative MAE: 0.1505360944496322\n",
            "Testing maxBins = 125, maxDepth = 20\n",
            "maxBins: 125, maxDepth: 20, MAE: 2064152.3547629095, MSE: 24743750256187.19, R²: 0.9209706569322761, MAPE: 0.27563302791465194, Relative MAE: 0.1505500052067136\n",
            "Testing maxBins = 125, maxDepth = 22\n",
            "maxBins: 125, maxDepth: 22, MAE: 2064140.9714897282, MSE: 24743786114547.715, R²: 0.9209705424038539, MAPE: 0.2756347888254905, Relative MAE: 0.15054917496187592\n",
            "Testing maxBins = 125, maxDepth = 24\n",
            "maxBins: 125, maxDepth: 24, MAE: 2064141.0139733886, MSE: 24743792034099.867, R²: 0.9209705234973294, MAPE: 0.2756351035644132, Relative MAE: 0.15054917806044338\n",
            "Testing maxBins = 125, maxDepth = 26\n",
            "maxBins: 125, maxDepth: 26, MAE: 2064141.0139733886, MSE: 24743788901058.688, R²: 0.9209705335039851, MAPE: 0.2756340392727243, Relative MAE: 0.15054917806044338\n",
            "Testing maxBins = 125, maxDepth = 28\n",
            "maxBins: 125, maxDepth: 28, MAE: 2064141.0139733886, MSE: 24743788901058.688, R²: 0.9209705335039851, MAPE: 0.2756340392727243, Relative MAE: 0.15054917806044338\n",
            "Testing maxBins = 125, maxDepth = 30\n",
            "maxBins: 125, maxDepth: 30, MAE: 2064141.0139733886, MSE: 24743788901058.688, R²: 0.9209705335039851, MAPE: 0.2756340392727243, Relative MAE: 0.15054917806044338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SOCkosgpzJfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import abs, col\n",
        "\n",
        "def spark_train_test_split(spark_df, test_size=0.2, seed=42):\n",
        "    # Convert Spark DataFrame to Pandas DataFrame\n",
        "    pandas_df = spark_df.toPandas()\n",
        "\n",
        "    # Perform train-test split using sklearn\n",
        "    train_df, test_df = train_test_split(pandas_df, test_size=test_size, random_state=seed)\n",
        "\n",
        "    # Convert Pandas DataFrame back to Spark DataFrame\n",
        "    train_spark_df = spark.createDataFrame(train_df)\n",
        "    test_spark_df = spark.createDataFrame(test_df)\n",
        "\n",
        "    return train_spark_df, test_spark_df\n",
        "\n",
        "merge_df_4 = merge_df\n",
        "\n",
        "# Drop 'EntityIndex' column if it exists\n",
        "if 'EntityIndex' in merge_df_4.columns:\n",
        "    merge_df_4 = merge_df_4.drop(\"EntityIndex\")\n",
        "\n",
        "# Convert 'Entity' to numerical index\n",
        "indexer = StringIndexer(inputCol=\"Entity\", outputCol=\"EntityIndex\")\n",
        "merge_df_4 = indexer.fit(merge_df_4).transform(merge_df_4)\n",
        "\n",
        "# Drop 'Code' column\n",
        "merge_df_4 = merge_df_4.drop(\"Code\")\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "if 'features' in merge_df_4.columns:\n",
        "    merge_df_4 = merge_df_4.drop(\"features\")\n",
        "feature_cols = [col for col in merge_df_4.columns if col not in ['in_tour_arrivals_ovn_vis_tourists', 'Entity', 'Year']]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "merge_df_4 = assembler.transform(merge_df_4)\n",
        "\n",
        "# Split data into training and testing sets (80/20)\n",
        "train_df, test_df = spark_train_test_split(merge_df_4, test_size=0.2, seed=42)\n",
        "\n",
        "# Initialize lists to store results\n",
        "results = []\n",
        "\n",
        "# Testing maxBins from 60 to 125\n",
        "for maxBins in range(60, 130, 5):\n",
        "    print(f\"Testing maxBins = {maxBins}\")\n",
        "\n",
        "    # Train Random Forest Regressor\n",
        "    rf = RandomForestRegressor(featuresCol=\"features\",\n",
        "                               labelCol=\"in_tour_arrivals_ovn_vis_tourists\",\n",
        "                               numTrees=100,\n",
        "                               maxBins=maxBins,\n",
        "                               maxDepth=30,\n",
        "                               seed=42)\n",
        "    rf_model = rf.fit(train_df)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = rf_model.transform(test_df)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "    mae = evaluator.evaluate(predictions)\n",
        "\n",
        "    mse_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "    mse = mse_evaluator.evaluate(predictions)\n",
        "\n",
        "    r2_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "    r2 = r2_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Calculate MAPE\n",
        "    predictions = predictions.withColumn(\"absolute_percentage_error\",\n",
        "                                         (abs(col(\"in_tour_arrivals_ovn_vis_tourists\") - col(\"prediction\")) / col(\"in_tour_arrivals_ovn_vis_tourists\")))\n",
        "    mape = predictions.select(\"absolute_percentage_error\").agg({\"absolute_percentage_error\": \"avg\"}).collect()[0][0]\n",
        "\n",
        "    # Calculate Relative MAE\n",
        "    mean_y_true = test_df.select(\"in_tour_arrivals_ovn_vis_tourists\").agg({\"in_tour_arrivals_ovn_vis_tourists\": \"avg\"}).collect()[0][0]\n",
        "    relative_mae = mae / mean_y_true\n",
        "\n",
        "    # Store results\n",
        "    results.append((maxBins, mae, mse, r2, mape, relative_mae))\n",
        "\n",
        "    # Print results for the current maxBins\n",
        "    print(f\"maxBins: {maxBins}, MAE: {mae}, MSE: {mse}, R²: {r2}, MAPE: {mape}, Relative MAE: {relative_mae}\")\n",
        "\n",
        "# Convert results to a DataFrame for easy viewing\n",
        "results_df = spark.createDataFrame(results, [\"maxBins\", \"MAE\", \"MSE\", \"R2\", \"MAPE\", \"Relative_MAE\"])\n",
        "results_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p87ssVNyYKJh",
        "outputId": "7e4c4a41-b1f8-444b-de90-dcc467194559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing maxBins = 60\n",
            "maxBins: 60, MAE: 1595603.2811390422, MSE: 10218686207667.1, R²: 0.9744289165534339, MAPE: 0.19414449226210942, Relative MAE: 0.11362332301968465\n",
            "Testing maxBins = 65\n",
            "maxBins: 65, MAE: 1660009.8796470526, MSE: 10286590257891.904, R²: 0.9742589945008949, MAPE: 0.20684924312700237, Relative MAE: 0.11820973358512965\n",
            "Testing maxBins = 70\n",
            "maxBins: 70, MAE: 1627757.0845196294, MSE: 10876716071656.346, R²: 0.972782272726581, MAPE: 0.20760579286731098, Relative MAE: 0.11591300368843827\n",
            "Testing maxBins = 75\n",
            "maxBins: 75, MAE: 1577911.264116084, MSE: 9605113977677.867, R²: 0.9759643102796625, MAPE: 0.20126166943103382, Relative MAE: 0.11236346990404408\n",
            "Testing maxBins = 80\n",
            "maxBins: 80, MAE: 1603978.7683580627, MSE: 10284848699911.096, R²: 0.9742633525488424, MAPE: 0.20770751226602224, Relative MAE: 0.11421974363437196\n",
            "Testing maxBins = 85\n",
            "maxBins: 85, MAE: 1611278.8642264456, MSE: 9812810332431.021, R²: 0.9754445741109409, MAPE: 0.2064642668326854, Relative MAE: 0.11473958535238084\n",
            "Testing maxBins = 90\n",
            "maxBins: 90, MAE: 1643591.1235417943, MSE: 10102082621986.68, R²: 0.9747207035756604, MAPE: 0.20357691958310437, Relative MAE: 0.11704054970929967\n",
            "Testing maxBins = 95\n",
            "maxBins: 95, MAE: 1555697.8107677638, MSE: 9448241664443.186, R²: 0.9763568651473489, MAPE: 0.19746965999541571, Relative MAE: 0.11078164413631496\n",
            "Testing maxBins = 100\n",
            "maxBins: 100, MAE: 1682747.1336051503, MSE: 10407144304352.508, R²: 0.9739573219062758, MAPE: 0.20635196899166527, Relative MAE: 0.11982885932998102\n",
            "Testing maxBins = 105\n",
            "maxBins: 105, MAE: 1696582.3308706314, MSE: 10688139502213.273, R²: 0.9732541638381486, MAPE: 0.21747442433680372, Relative MAE: 0.12081406731151301\n",
            "Testing maxBins = 110\n",
            "maxBins: 110, MAE: 1624267.1643735948, MSE: 10391469126088.504, R²: 0.9739965472316535, MAPE: 0.20245472102678524, Relative MAE: 0.11566448557071246\n",
            "Testing maxBins = 115\n",
            "maxBins: 115, MAE: 1690747.0937815926, MSE: 11980808555560.383, R²: 0.97001940864945, MAPE: 0.20930921573319253, Relative MAE: 0.12039853856667927\n",
            "Testing maxBins = 120\n",
            "maxBins: 120, MAE: 1630191.0535050072, MSE: 9958029613786.266, R²: 0.975081179611284, MAPE: 0.21063036718494432, Relative MAE: 0.11608632725045052\n",
            "Testing maxBins = 125\n",
            "maxBins: 125, MAE: 1612679.320378909, MSE: 9245967176426.713, R²: 0.9768630337200074, MAPE: 0.21744537515527365, Relative MAE: 0.11483931219781114\n",
            "+-------+------------------+--------------------+------------------+-------------------+-------------------+\n",
            "|maxBins|               MAE|                 MSE|                R2|               MAPE|       Relative_MAE|\n",
            "+-------+------------------+--------------------+------------------+-------------------+-------------------+\n",
            "|     60|1595603.2811390422| 1.02186862076671E13|0.9744289165534339|0.19414449226210942|0.11362332301968465|\n",
            "|     65|1660009.8796470526|1.028659025789190...|0.9742589945008949|0.20684924312700237|0.11820973358512965|\n",
            "|     70|1627757.0845196294|1.087671607165634...| 0.972782272726581|0.20760579286731098|0.11591300368843827|\n",
            "|     75| 1577911.264116084|9.605113977677867E12|0.9759643102796625|0.20126166943103382|0.11236346990404408|\n",
            "|     80|1603978.7683580627|1.028484869991109...|0.9742633525488424|0.20770751226602224|0.11421974363437196|\n",
            "|     85|1611278.8642264456|9.812810332431021E12|0.9754445741109409| 0.2064642668326854|0.11473958535238084|\n",
            "|     90|1643591.1235417943|1.010208262198668E13|0.9747207035756604|0.20357691958310437|0.11704054970929967|\n",
            "|     95|1555697.8107677638|9.448241664443186E12|0.9763568651473489|0.19746965999541571|0.11078164413631496|\n",
            "|    100|1682747.1336051503|1.040714430435250...|0.9739573219062758|0.20635196899166527|0.11982885932998102|\n",
            "|    105|1696582.3308706314|1.068813950221327...|0.9732541638381486|0.21747442433680372|0.12081406731151301|\n",
            "|    110|1624267.1643735948|1.039146912608850...|0.9739965472316535|0.20245472102678524|0.11566448557071246|\n",
            "|    115|1690747.0937815926|1.198080855556038...|  0.97001940864945|0.20930921573319253|0.12039853856667927|\n",
            "|    120|1630191.0535050072|9.958029613786266E12| 0.975081179611284|0.21063036718494432|0.11608632725045052|\n",
            "|    125| 1612679.320378909|9.245967176426713E12|0.9768630337200074|0.21744537515527365|0.11483931219781114|\n",
            "+-------+------------------+--------------------+------------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import abs, col\n",
        "\n",
        "def spark_train_test_split(spark_df, test_size=0.2, seed=42):\n",
        "    # Convert Spark DataFrame to Pandas DataFrame\n",
        "    pandas_df = spark_df.toPandas()\n",
        "\n",
        "    # Perform train-test split using sklearn\n",
        "    train_df, test_df = train_test_split(pandas_df, test_size=test_size, random_state=seed)\n",
        "\n",
        "    # Convert Pandas DataFrame back to Spark DataFrame\n",
        "    train_spark_df = spark.createDataFrame(train_df)\n",
        "    test_spark_df = spark.createDataFrame(test_df)\n",
        "\n",
        "    return train_spark_df, test_spark_df\n",
        "\n",
        "merge_df_4 = merge_df\n",
        "\n",
        "# Drop 'EntityIndex' column if it exists\n",
        "if 'EntityIndex' in merge_df_4.columns:\n",
        "    merge_df_4 = merge_df_4.drop(\"EntityIndex\")\n",
        "\n",
        "# Convert 'Entity' to numerical index\n",
        "indexer = StringIndexer(inputCol=\"Entity\", outputCol=\"EntityIndex\")\n",
        "merge_df_4 = indexer.fit(merge_df_4).transform(merge_df_4)\n",
        "\n",
        "# Drop 'Code' column\n",
        "merge_df_4 = merge_df_4.drop(\"Code\")\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "if 'features' in merge_df_4.columns:\n",
        "    merge_df_4 = merge_df_4.drop(\"features\")\n",
        "feature_cols = [col for col in merge_df_4.columns if col not in ['in_tour_arrivals_ovn_vis_tourists', 'Entity', 'Year']]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "merge_df_4 = assembler.transform(merge_df_4)\n",
        "\n",
        "# Split data into training and testing sets (80/20)\n",
        "train_df, test_df = spark_train_test_split(merge_df_4, test_size=0.2, seed=42)\n",
        "\n",
        "# Initialize lists to store results\n",
        "results = []\n",
        "\n",
        "# Testing maxDepth from 10 to 30\n",
        "for maxDepth in range(10, 32, 2):\n",
        "    print(f\"Testing maxBins = {maxBins}, maxDepth = {maxDepth}\")\n",
        "\n",
        "    # Train Random Forest Regressor\n",
        "    rf = RandomForestRegressor(featuresCol=\"features\",\n",
        "                                labelCol=\"in_tour_arrivals_ovn_vis_tourists\",\n",
        "                                numTrees=100,\n",
        "                                maxBins=75,\n",
        "                                maxDepth=maxDepth,\n",
        "                                seed=42)\n",
        "    rf_model = rf.fit(train_df)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = rf_model.transform(test_df)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "    mae = evaluator.evaluate(predictions)\n",
        "\n",
        "    mse_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "    mse = mse_evaluator.evaluate(predictions)\n",
        "\n",
        "    r2_evaluator = RegressionEvaluator(labelCol=\"in_tour_arrivals_ovn_vis_tourists\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "    r2 = r2_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Calculate MAPE\n",
        "    predictions = predictions.withColumn(\"absolute_percentage_error\",\n",
        "                                          (abs(col(\"in_tour_arrivals_ovn_vis_tourists\") - col(\"prediction\")) / col(\"in_tour_arrivals_ovn_vis_tourists\")))\n",
        "    mape = predictions.select(\"absolute_percentage_error\").agg({\"absolute_percentage_error\": \"avg\"}).collect()[0][0]\n",
        "\n",
        "    # Calculate Relative MAE\n",
        "    mean_y_true = test_df.select(\"in_tour_arrivals_ovn_vis_tourists\").agg({\"in_tour_arrivals_ovn_vis_tourists\": \"avg\"}).collect()[0][0]\n",
        "    relative_mae = mae / mean_y_true\n",
        "\n",
        "    # Store results\n",
        "    results.append((maxBins, maxDepth, mae, mse, r2, mape, relative_mae))\n",
        "\n",
        "    # Print results for the current maxBins and maxDepth\n",
        "    print(f\"maxBins: {maxBins}, maxDepth: {maxDepth}, MAE: {mae}, MSE: {mse}, R²: {r2}, MAPE: {mape}, Relative MAE: {relative_mae}\")\n",
        "\n",
        "# Convert results to a DataFrame for easy viewing\n",
        "results_df = spark.createDataFrame(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgbcUpU4YKdB",
        "outputId": "df0b827e-3c70-4526-9021-faa1eaaefeb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing maxBins = 125, maxDepth = 10\n",
            "maxBins: 125, maxDepth: 10, MAE: 1644076.6871784877, MSE: 10059583605994.227, R²: 0.9748270524606594, MAPE: 0.2247651606588108, Relative MAE: 0.11707512682166261\n",
            "Testing maxBins = 125, maxDepth = 12\n",
            "maxBins: 125, maxDepth: 12, MAE: 1599540.0563606366, MSE: 9693892773071.996, R²: 0.975742151585367, MAPE: 0.2076387299880793, Relative MAE: 0.11390366180310693\n",
            "Testing maxBins = 125, maxDepth = 14\n",
            "maxBins: 125, maxDepth: 14, MAE: 1585510.7529450022, MSE: 9629619410403.162, R²: 0.9759029882611193, MAPE: 0.20282911799799017, Relative MAE: 0.11290463147233583\n",
            "Testing maxBins = 125, maxDepth = 16\n",
            "maxBins: 125, maxDepth: 16, MAE: 1579347.8950757068, MSE: 9608411634742.219, R²: 0.9759560582732638, MAPE: 0.2015944312769817, Relative MAE: 0.11246577276686395\n",
            "Testing maxBins = 125, maxDepth = 18\n",
            "maxBins: 125, maxDepth: 18, MAE: 1578343.0036610886, MSE: 9606599976523.125, R²: 0.9759605917389712, MAPE: 0.20136686734087142, Relative MAE: 0.11239421418889378\n",
            "Testing maxBins = 125, maxDepth = 20\n",
            "maxBins: 125, maxDepth: 20, MAE: 1577862.2829865792, MSE: 9605222812142.514, R²: 0.9759640379339694, MAPE: 0.2012593387974194, Relative MAE: 0.1123599819451226\n",
            "Testing maxBins = 125, maxDepth = 22\n",
            "maxBins: 125, maxDepth: 22, MAE: 1577893.0286777029, MSE: 9605134015725.232, R²: 0.9759642601367604, MAPE: 0.20126331805385034, Relative MAE: 0.11236217135375275\n",
            "Testing maxBins = 125, maxDepth = 24\n",
            "maxBins: 125, maxDepth: 24, MAE: 1577911.264116084, MSE: 9605113977677.867, R²: 0.9759643102796625, MAPE: 0.20126166943103382, Relative MAE: 0.11236346990404408\n",
            "Testing maxBins = 125, maxDepth = 26\n",
            "maxBins: 125, maxDepth: 26, MAE: 1577911.264116084, MSE: 9605113977677.867, R²: 0.9759643102796625, MAPE: 0.20126166943103382, Relative MAE: 0.11236346990404408\n",
            "Testing maxBins = 125, maxDepth = 28\n",
            "maxBins: 125, maxDepth: 28, MAE: 1577911.264116084, MSE: 9605113977677.867, R²: 0.9759643102796625, MAPE: 0.20126166943103382, Relative MAE: 0.11236346990404408\n",
            "Testing maxBins = 125, maxDepth = 30\n",
            "maxBins: 125, maxDepth: 30, MAE: 1577911.264116084, MSE: 9605113977677.867, R²: 0.9759643102796625, MAPE: 0.20126166943103382, Relative MAE: 0.11236346990404408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rsiDLNUVYj0Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}